\documentclass[a4paper]{article}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage[round]{natbib}

% working title, please change
\title{Python as a machine learning language:
       experiences from the scikit-learn project}

% Dutch name sorting hack as per http://tex.stackexchange.com/a/40750/2806
\DeclareRobustCommand{\VAN}[3]{#2}

\newcommand{\Xtr}{\texttt{X\_tr}}
\newcommand{\Xte}{\texttt{X\_te}}
\newcommand{\ytr}{\texttt{y\_tr}}
\newcommand{\yte}{\texttt{y\_te}}

\begin{document}

\maketitle

\section{Project vision}

\subsection{General principles}

scikit-learn \citep{pedregosa2011} is an open source machine learning library
for the Python programming language,
intended to tie in with the set of numeric and scientific packages
centered around the NumPy and SciPy libraries
\Citep{vanderwalt2011, varoquaux2013scipy}.
It is a library, rather than a novel (domain-specific) programming language,
intended to be called from Python programs;
at present, it offers no command line interface or GUI.

The main goal of the scikit-learn project
is to provide high-quality implementations
of ``classic'' machine learning algorithms
with a consistent API.

\subsection{Data formats}

For a machine learning library, one of the most important decisions we had to
make is how to represent data.  Rather than reinventing the wheel, we opted for
NumPy multidimensional arrays \Citep{vanderwalt2011} for dense data and SciPy
sparse matrices for sparse data.  While those may seem like a ``bare''
representation of data when compared to more object-oriented representations
(Weka is an example of this style of representation, \citealp{hall2009weka}),
it brings the prime advantage of allowing us to rely on NumPy and SciPy's
\textit{vector operations} which are often orders of magnitude faster
than the corresponding Python loops,
while at the same time keeping the code short and readable.
Conversion to NumPy and SciPy formats is usually easy and many scientific users
of Python will already be used to such formats, since they are pervasive in
other scientific Python packages.  For tasks where input is likely to consist
of text files or semi-structured objects, we provide ``vectorizers'' -- objects
that efficiently convert such data to the NumPy or SciPy formats.
% FIXME: elaborate what is meant below
% Lars: elaborated, but I'm not sure how important this is.
% It mostly reflects experiences with OO libraries that require conversions
% between types just to pass information to a method.
%Finally, offering a custom datatype for samples would require a conversion
%\textit{in any case}, since such a datatype would be toolkit-specific
%and no program would store its data using such a type
%if it weren't from the outset designed to use scikit-learn.

In the remainder of the paper, we use the following notation. Training data are
denoted by \Xtr ~and test data (i.e., unseen data to which the learning
algorithm need be able to generalize) by \Xte.  Following the discussion above,
we represent \Xtr ~and \Xte ~by 2-dimensional NumPy arrays or SciPy sparse
matrices. Using NumPy/SciPy's notation, \Xtr\texttt{[i, j]} denotes the
$i^\textrm{th}$ training sample's $j^\textrm{th}$ feature. For supervised learning tasks,
such as classification or regression, we denote the training and test labels by
\ytr ~and \yte, respectively. We typically store \ytr ~and \yte ~as
1-dimensional NumPy arrays (\ytr\texttt{[i]} denotes the $i^\textrm{th}$ training
sample's label value).

\section{Application programming interface (API)}

\subsection{Estimators}

% TODO describe transformer vs. classifiers/regressors

The bulk of the scikit-learn application programming interface
consists of so-called \textit{estimator} objects.
All learning algorithms for classification, regression and clustering
are offered as such objects,
but so are feature extraction, feature selection and dimensionality reduction.

Construction and application of an estimator object
proceeds in a way that is somewhat reminiscent of partial function application.
The constructor, \texttt{\_\_init\_\_}\footnote{
  Strictly speaking, \texttt{\_\_init\_\_} is Python's
  class \textit{initialization} method,
  while \texttt{\_\_new\_\_} is the constructor.
  The distinction is not relevant to the present description of scikit-learn.}
takes a set of named (hyper-)parameters
and attaches these to the estimator as public attributes.\footnote{
  The Python language does not actually distinguish between public and private
  attributes and methods,
  but by convention, any identifier prefixed with an underscore (\texttt{\_})
  is considered ``a non-public part of the API\ldots
  an implementation detail and subject to change without notice.''
  \Citep{pythontut}
  }
It never performs actual learning, nor does it even see a training set.

The workhorse of most estimators, and one of the few methods
that all estimators have in common, is called \texttt{fit}.
Calling this method causes the estimator to learn from a training set,
supplied as one or more arguments.
Its task, from an API point of view,
is to determine (via a learning algorithm) model-specific parameters
from a training set and set these as attributes on the estimator object,
which can then be used to transform data or make predictions.
As in other object-oriented machine learning libraries
(e.g.\ Weka's Java API)
no distinction is made in the type hierarchy
between what is variously called an estimator or a \textit{learner}
in the machine learning literature,
and the \textit{model} that results from applying an estimator to data.
When the \texttt{fit} method has been called,
an estimator object serves dual purpose as a model
(and is referred to as such in the documentation).

An example to clarify the partial application style
is the following code snippet.
If $X$ is a dataset in the format described below and $k$ is some integer,
then the Python code to train a $k$-means clustering model on it
can be written as
% XXX we could use Pygments for syntax highlighting
\begin{verbatim}
km = KMeans(n_clusters=k).fit(X)
\end{verbatim}
This snippet relies on the fact that the \texttt{fit} method
always returns the object it was called on (its ``\texttt{self}''),
making a method chaining style possible.

Importantly, the parameters learned by an estimator
are exposed as \textit{public} attributes on the trained object.
This facilitates model inspection
and makes it possible to train a model using scikit-learn,
export it to a (standardized or custom) external data format
and use the learned parameters in a different piece of code,
perhaps a prediction algorithm implemented in a different language
or one that combines the model parameters with other information
to build a custom model.
Of course, it also forces the developers to think carefully
about what might in a more ``black box''-style toolkit
be considered the internals of objects,
since authors are client code are allowed to rely
on the names and formats of model parameters.

A \texttt{fit} method is present even on objects
that perform seemingly mundane tasks such as vectorizing text documents
for subsequent learning.
Such classes, however, can be said to learn their vocabulary
from the training corpus,
and follow the conventions set out above.
Even those estimators that are entirely stateless and therefore
do not require a \texttt{fit} method to perform useful work,
nevertheless have such a method for consistency and composibility,
and to perform parameter validation.
Examples of this latter kind of objects are the normalizing transformer,
kernel approximation transformers
\citep{rahimi2007random, li2010random, vedaldi2010efficient},
and two feature extraction classes that implement the ``hashing trick''
\citep{weinberger2009}.

\section{Pipelines and model selection}

A distinguishing feature of scikit-learn estimators
is that they can be composed into \texttt{Pipeline} objects.
Not only do such objects combine typical machine learning workflows
(extracting features, centering/scaling/normalization,
feature selection, making predictions)
in a single object that is itself an estimator,
and whose methods cause data to flow through the pipeline.
More importantly, though, they can be used for
systematic, automated model selection.
This is best explained by example.

By composing, say, a \texttt{CountVectorizer} object
(which extracts term frequency features from text documents
according to a bag-words model),
a \textsf{tf--idf} transformer, a $\chi^2$ feature selector and a linear SVM,
one obtains a pipeline object with various hyperparameters:
whether to do stopword filtering,
linear vs.\ logarithmic \textsf{tf}, number of features to keep,
SVM regularization ($L_1$ or $L_2$ norm, strength $C$) a.o.
Such a composite \texttt{Pipeline} object
exposes all of these parameters using a special syntax;
each estimators must be given a name,
and if the SVM's name is, e.g., \texttt{linearsvm},
then its $C$ parameter is exposed as \texttt{linearsvm\_\_C}.

The aforementioned \texttt{Pipeline} object
can be passed as an argument to a model selection algorithm.
Available algorithms at the time of writing include random search
\citep{bergstra2012} and ``grid search''.
The latter, which we shall use in this example,
performs an exhaustive search through a grid formed by the Cartesian product
of a set of possible values for each parameter, specified by the user.
In our example, this grid might be given as

\begin{align*}
         & \textsf{stopwords} \in \{0, 1\}                      \\
  \times & \; \textsf{tf} \in \{\textsf{linear}, \textsf{log}\} \\
  \times & \; n_\textsf{features} \in \{1000, 2000, 5000\}      \\
  \times & \; \textsf{norm} \in \{L_1, L_2\}                    \\
  \times & \; C \in \{10, 100, 1000\}
\end{align*}

At each point in this grid, $k$-fold cross validation is run
to estimate the performance of the estimator according to some measure
(e.g.\ accuracy, $F_\beta$-score or a host of clustering quality metrics)
and the best performing set of hyperparameters is stored
on the \texttt{GridSearchCV} object; again, as a public attribute.

\section{Implementation}

scikit-learn is primarily implemented in Python and Cython
\citep{behnel2011cython},
a language that extends Python with static typing
and a compiler that produces C extension modules
for the Python runtime system.
In addition, it includes (modified versions of)
the \textsf{LIBSVM} and \textsf{LIBLINEAR} libraries
used for training support vector machines
and logistic regression models \citep{chang2011libsvm, fan2008}.
These libraries, both written in C{}\verb!++!,
and wrapped using Cython modules.

As much as possible is implemented in ``pure'' Python,
to avoid the compile-edit-debug cycle needed for Cython
and to keep the codebase readable for as large an audience as possible.
However, since the speed of the current Python implementation
is not sufficient for numeric programming
and NumPy's vector operations are not appropriate for all use cases
(among other factors, because they often have linear space requirements),
some of the core algorithms have to be implemented in Cython.\footnote{
  The only Python implementation currently supported by scikit-learn
  is the reference implementation, CPython.
  The alternative Python implementation PyPy \citep{bolz2009tracing}
  promises to solve this problem by JIT compilation,
  but does not yet interoperate with NumPy and SciPy.
}
Examples include the aforementioned $k$-means implementation,
stochastic gradient descent for linear models
and some graph-based clustering algorithms.

While object-oriented techniques and design patterns
are used throughout scikit-learn,
they mostly serve to facilitate code reuse and are not considered a project goal.
Where possible, the Python principle of ``duck typing'' is exploited
to simplify the implementation and skip the introduction of superfluous classes.
This allows for extensibility and flexibility at the same time:
user-defined estimators that follow scikit-learn conventions
should be usable in pipelines and other composite objects
without them actually inheriting from scikit-learn base classes.

\section{Comparison with other packages}

ToDo.

\section{Future work}

ToDo.

\bibliographystyle{plainnat}
\DeclareRobustCommand{\VAN}[3]{#3}
\bibliography{paper}

\end{document}
