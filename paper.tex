\documentclass[a4paper]{article}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage[round]{natbib}

% working title, please change
\title{Python as a machine learning language:
       experiences from the scikit-learn project}

% Add your name and affiliation here.
% Don't worry too much about the formatting,
% as we'll have to redo that for the LNCS format.
\author{Mathieu Blondel \\
        Graduate School of System Informatics \\
        Kobe University \\
        \small{\texttt{mblondel@ai.cs.kobe-u.ac.jp}}
      \and
        Lars Buitinck \\
        Informatics Institute \\
        University of Amsterdam \\
        \small{\texttt{L.J.Buitinck@uva.nl}}
      \and
        Jake Vanderplas \\
        Astronomy Department \\
        University of Washington \\
        \small{\texttt{vanderplas@astro.washington.edu}}
}

% Dutch name sorting hack as per http://tex.stackexchange.com/a/40750/2806
\DeclareRobustCommand{\VAN}[3]{#2}

\newcommand{\Xtr}{\texttt{X\_tr}}
\newcommand{\Xte}{\texttt{X\_te}}
\newcommand{\ytr}{\texttt{y\_tr}}
\newcommand{\yte}{\texttt{y\_te}}

\begin{document}

\maketitle

\begin{abstract}
The scikit-learn project aims to create an open source
machine learning library for the Python programming language,
offering performant implementations of well-established learning algorithms,
and tying in with the ecosystem of scientific software
that has evolved around the NumPy and SciPy packages.
Since the start of the project in 2009,
it has gathered a sizeable user base.

This paper discusses design choices made
in both the programmers' interface (API) and the implementation
of scikit-learn.
It also highlights obstacles faced by users and implementers
of machine learning algorithms in the Python language.
\end{abstract}

\section{Project vision}

\subsection{General principles}

% XXX the following is more of a general introduction than a project vision
scikit-learn \citep{pedregosa2011} is an open source machine learning library
for the Python programming language,
intended to tie in with the set of numeric and scientific packages
centered around the NumPy and SciPy libraries
\Citep{vanderwalt2011, varoquaux2013scipy}.
It is a library, rather than a novel (domain-specific) programming language,
intended to be called from Python programs;
at present, it offers no command line interface or GUI.

The scikit-learn project started in 2009 and has since been developed
by a team largely consisting of scientists
(both from computer science and other fields).
Its popularity can be gauged from various sources:
% TODO: list volume on the mailing list, number of contributors,
% number of issues opened and closed, number of forks, number of surveys filled in.
at the time of writing,
the popular programmer's Q~\&{}A site StackOverflow\footnote{
  \url{http://stackoverflow.com/questions/tagged/scikit-learn}}
% update the following figure when paper is finalized
lists 322 scikit-learn-related questions.
The version control logs for the development version of scikit-learn
% ddaa494c116e3c16bf032003c5cccbed851733d2
show 183 contributors to the project's source code and documentation
(after manual deduplication).

The main goal of the scikit-learn project
is to provide high-quality implementations
of ``classic'' machine learning algorithms
with a consistent API.

\subsection{Data formats}

For a machine learning library, one of the most important decisions we had to
make is how to represent data.  Rather than reinventing the wheel, we opted for
NumPy multidimensional arrays \Citep{vanderwalt2011} for dense data and SciPy
sparse matrices for sparse data.  While those may seem like a ``bare''
representation of data when compared to more object-oriented representations
(Weka is an example of this style of representation, \citealp{hall2009weka}),
it brings the prime advantage of allowing us to rely on NumPy and SciPy's
\textit{vector operations} which are often orders of magnitude faster
than the corresponding Python loops,
while at the same time keeping the code short and readable.
Conversion to NumPy and SciPy formats is usually easy and many scientific users
of Python will already be used to such formats, since they are pervasive in
other scientific Python packages.  For tasks where input is likely to consist
of text files or semi-structured objects, we provide ``vectorizers'' -- objects
that efficiently convert such data to the NumPy or SciPy formats.
% FIXME: elaborate what is meant below
% Lars: elaborated, but I'm not sure how important this is.
% It mostly reflects experiences with OO libraries that require conversions
% between types just to pass information to a method.
%Finally, offering a custom datatype for samples would require a conversion
%\textit{in any case}, since such a datatype would be toolkit-specific
%and no program would store its data using such a type
%if it weren't from the outset designed to use scikit-learn.

In the remainder of the paper, we use the following notation. Training data are
denoted by \Xtr ~and test data (i.e., unseen data to which the learning
algorithm need be able to generalize) by \Xte.  Following the discussion above,
we represent \Xtr ~and \Xte ~by 2-dimensional NumPy arrays or SciPy sparse
matrices. Using NumPy/SciPy's notation, \Xtr\texttt{[i, j]} denotes the
$i^\textrm{th}$ training sample's $j^\textrm{th}$ feature. For supervised learning tasks,
such as classification or regression, we denote the training and test labels by
\ytr ~and \yte, respectively. We typically store \ytr ~and \yte ~as
1-dimensional NumPy arrays (\ytr\texttt{[i]} denotes the $i^\textrm{th}$ training
sample's label value).

\section{Application programming interface (API)}

\subsection{Estimators}

% TODO describe transformer vs. classifiers/regressors

The bulk of the scikit-learn application programming interface
consists of so-called \textit{estimator} objects.
All learning algorithms for classification, regression and clustering
are offered as such objects,
but so are feature extraction, feature selection and dimensionality reduction.

Construction and application of an estimator object
proceeds in a way that is somewhat reminiscent of partial function application.
The constructor, \texttt{\_\_init\_\_}\footnote{
  Strictly speaking, \texttt{\_\_init\_\_} is Python's
  class \textit{initialization} method,
  while \texttt{\_\_new\_\_} is the constructor.
  The distinction is not relevant to the present description of scikit-learn.}
takes a set of named (hyper-)parameters
and attaches these to the estimator as public attributes.\footnote{
  The Python language does not actually distinguish between public and private
  attributes and methods,
  but by convention, any identifier prefixed with an underscore (\texttt{\_})
  is considered ``a non-public part of the API\ldots
  an implementation detail and subject to change without notice.''
  \Citep{pythontut}
  }
It never performs actual learning, nor does it even see a training set.

The workhorse of most estimators, and one of the few methods
that all estimators have in common, is called \texttt{fit}.
Invoking this method causes the estimator to learn from a training set,
supplied as one or more arguments.
Its task, from an API point of view,
is to determine (via a learning algorithm) model-specific parameters
from a training set and set these as attributes on the estimator object,
which can then be used to transform data or make predictions.
As in other object-oriented machine learning libraries
(e.g.\ Weka's Java API)
no distinction is made in the type hierarchy
between what is variously called an estimator or a \textit{learner}
in the machine learning literature,
and the \textit{model} that results from applying an estimator to data.
When the \texttt{fit} method has been called,
an estimator object serves dual purpose as a model
(and is referred to as such in the documentation).

An example to clarify the partial application style
is the following code snippet.
If $X$ is a dataset in the format described below and $k$ is some integer,
then the Python code to train a $k$-means clustering model on it
can be written as
% XXX we could use Pygments for syntax highlighting
\begin{verbatim}
km = KMeans(n_clusters=k).fit(X)
\end{verbatim}
This snippet relies on the fact that the \texttt{fit} method
always returns the object it was called on (its ``\texttt{self}''),
making a method chaining style possible.

Importantly, the parameters learned by an estimator
are exposed as \textit{public} attributes on the trained object.
This facilitates model inspection
and makes it possible to train a model using scikit-learn,
export it to a (standardized or custom) external data format
and use the learned parameters in a different piece of code,
perhaps a prediction algorithm implemented in a different language
or one that combines the model parameters with other information
to build a custom model.
Of course, it also forces the developers to think carefully
about what might in a more ``black box''-style toolkit
be considered the internals of objects,
since authors are client code are allowed to rely
on the names and formats of model parameters.

A \texttt{fit} method is present even on objects
that perform seemingly mundane tasks such as vectorizing text documents
for subsequent learning.
Such classes, however, can be said to learn their vocabulary
from the training corpus,
and follow the conventions set out above.
Even those estimators that are entirely stateless and therefore
do not require a \texttt{fit} method to perform useful work,
nevertheless have such a method for consistency and composibility,
and to perform parameter validation.
Examples of this latter kind of objects are the normalizing transformer,
kernel approximation transformers
\citep{rahimi2007random, li2010random, vedaldi2010efficient},
and two feature extraction classes that implement the ``hashing trick''
\citep{weinberger2009}.

\section{Pipelines and model selection}

A distinguishing feature of scikit-learn estimators
is that they can be composed into \texttt{Pipeline} objects.
Not only do such objects combine typical machine learning workflows
(extracting features, centering/scaling/normalization,
feature selection, making predictions)
in a single object that is itself an estimator,
and whose methods cause data to flow through the pipeline.
More importantly, though, they can be used for
systematic, automated model selection.
This is best explained by example.

By composing, say, a \texttt{CountVectorizer} object
(which extracts term frequency features from text documents
according to a bag-words model),
a \textsf{tf--idf} transformer, a $\chi^2$ feature selector and a linear SVM,
one obtains a pipeline object with various hyperparameters:
whether to do stopword filtering,
linear vs.\ logarithmic \textsf{tf}, number of features to keep,
SVM regularization ($L_1$ or $L_2$ norm, strength $C$) a.o.
Such a composite \texttt{Pipeline} object
exposes all of these parameters using a special syntax;
each estimators must be given a name,
and if the SVM's name is, e.g., \texttt{linearsvm},
then its $C$ parameter is exposed as \texttt{linearsvm\_\_C}.
The pipeline's \texttt{set\_params} splits the parameter names
on double underscores, uses the first part of the result
to look up the estimator,
and passes the second part to the estimator's own \texttt{set\_params}
as a keyword argument; due to Python's dynamic parameter passing,
the result is the same as if the user had called \texttt{set\_params}
on the SVM themselves.
(The double underscore syntax is not special to pipelines;
it is also employed in ensemble estimators such as random forests.)

The aforementioned \texttt{Pipeline} object
can be passed as an argument to a model selection algorithm.
Available algorithms at the time of writing include random search
\citep{bergstra2012} and ``grid search''.
The latter, which we shall use in this example,
performs an exhaustive search through a grid formed by the Cartesian product
of a set of possible values for each parameter, specified by the user.
In our example, this grid might be given as

\begin{align*}
         & \textsf{stopwords} \in \{0, 1\}                      \\
  \times & \; \textsf{tf} \in \{\textsf{linear}, \textsf{log}\} \\
  \times & \; n_\textsf{features} \in \{1000, 2000, 5000\}      \\
  \times & \; \textsf{norm} \in \{L_1, L_2\}                    \\
  \times & \; C \in \{10, 100, 1000\}
\end{align*}

At each point in this grid, $k$-fold cross validation is run
to estimate the performance of the estimator according to some measure
(e.g.\ accuracy, $F_\beta$-score or a host of clustering quality metrics)
and the best performing set of hyperparameters is stored
on the \texttt{GridSearchCV} object; again, as a public attribute.

\section{Implementation}

scikit-learn is primarily implemented in Python and Cython
\citep{behnel2011cython},
a language that extends Python with static typing
and a compiler that produces C extension modules
for the Python runtime system.
In addition, it includes (modified versions of)
the \textsf{LIBSVM} and \textsf{LIBLINEAR} libraries
used for training support vector machines
and logistic regression models \citep{chang2011libsvm, fan2008}.
These libraries, both written in C{}\verb!++!,
and wrapped using Cython modules.

As much as possible is implemented in ``pure'' Python,
to avoid the compile-edit-debug cycle needed for Cython
and to keep the codebase readable for as large an audience as possible.
However, since the speed of the current Python implementation
is not sufficient for numeric programming
and NumPy's vector operations are not appropriate for all use cases
(among other factors, because they often have linear space requirements),
some of the core algorithms have to be implemented in Cython.\footnote{
  The only Python implementation currently supported by scikit-learn
  is the reference implementation, CPython.
  The alternative Python implementation PyPy \citep{bolz2009tracing}
  promises to alleviate this problem by JIT compilation,
  but does not yet interoperate with NumPy and SciPy.
}
Examples include the aforementioned $k$-means implementation,
stochastic gradient descent for linear models
and some graph-based clustering algorithms.

While object-oriented techniques and design patterns
are used throughout scikit-learn,
they mostly serve to facilitate code reuse and are not considered a project goal.
Where possible, the Python principle of ``duck typing'' is exploited
to simplify the implementation and skip the introduction of superfluous classes.
This allows for extensibility and flexibility at the same time:
user-defined estimators that follow scikit-learn conventions
should be usable in pipelines and other composite objects
without them actually inheriting from scikit-learn base classes.

\section{Weaknesses of the NumPy/SciPy environment}

% This needs an introductory paragraph, but the best I could come up with was:
As can be expected, the NumPy/SciPy environment is not completely flawless.

% The following hedge needs to be in here; we don't want to diss our peers
It should be noted that this section is meant
as a summary of challenges faced by scikit-learn development
and an attempt at constructive criticism.
There is considerable overlap between the communities
that develop or contribute to NumPy, SciPy, scikit-learn, and Cython,
so the issues raised here may eventually be resolved
by the scikit-learn developers themselves.

\subsection{Sparse matrix support}

SciPy's handling of sparse matrices is one source of confusion
for scikit-learn developers and users alike.
The \texttt{scipy.sparse} package offers quite full-featured support
for this kind of data structure,
which is crucial to achieving performance and scalability
in the face of high-dimensional inputs
(including, but not limited to, NLP applications).
However, the interface exposed by its classes
deviates from the NumPy array interface, sometimes subtly so,
sometimes entirely.  % XXX is this grammatically correct?
For example, while NumPy arrays have a variable number of dimensions,
so that vectors can be considered 1-d arrays
(without a distinction between row and column vectors),
matrices are 2-d, etc., \texttt{scipy.sparse} matrices are always 2-dimensional,
meaning that special care has to be taken when handling sparse vectors.

The development team has built up sufficient familiarity
with the sparse/array distinction to factor out common patterns
for supporting both sparse and dense input data structures in many estimators.
In other cases, though, sparse matrix support has to implemented
on a per-estimator basis.
This situation is aggravated by the lack of a C API for sparse matrices,
so that Cython modules have to implement their own version
of operations like sparse dot products
that ideally would have been handled by a library.

However, users of scikit-learn cannot always be expected
to have similar familiarity, and in some cases
(e.g., when users want to leverage feature extraction code
outside the context of a \texttt{Pipeline})
they will have to face the \texttt{scipy.sparse} interface.
This is, of course, as much an artifact of the design choice
to not implement separate classes and interfaces for samples/instances
as it a consequence of SciPy's implementation of sparse matrices.

\subsection{Parallelism}

At this point in time, support for multi-core programming in Python,
at least in the CPython reference implementation targeted by scikit-learn,
is downright weak.
The interpreter supports multithreading, but due to the ``Global Interpreter Lock'' (GIL),
no two threads in a process may at the same time execute Python code.
It follows that multithreading is not a viable solution
even two embarrassingly parallel problems,
such as fitting and evaluation multiple models in a grid search procedure.
The workaround is to use Python's multiprocessing module,
which emulates threads using multiple operating system-level processes,
but this in turn leads to high memory use and overhead
because of the copying of training and test sets
into the various processes' address spaces.\footnote{
  The fact that copying occurs at all is an artifact of NumPy's implementation;
  no attempt at implementing a multiprocessing module
  that shares NumPy arrays using copy-on-write semantics or memory mapping
  has so far been completed.}

\subsection{Python/Cython mismatch}

The Python/Cython divide is another source of potential difficulties
both for users and developers/contributors.
While Cython makes writing performance-critical code possible
in a language that is very much like Python,
doing so properly still requires some knowledge of C.
One source of trouble has been the various fixed-size integer types
that are used by the Python and NumPy C runtimes,
whose sizes are practically all platform-dependent.
Choosing the appropriate integer type from among
\texttt{int}, \texttt{size\_t}, \texttt{Py\_ssize\_t} and \texttt{npy\_intp}
can require knowledge of the Python extension API,
the NumPy C API and the C standard.
Neither Cython nor C compilers provide much in the way of static checking
for the right type,
while picking the wrong type may cause segmentation faults
when users try to use arrays that are too large for the type used
and using a dynamic type often leads to unacceptable slowdowns.

Another problem concerns the mismatch between on the one hand Python functions,
which are first-class runtime objects endowed with methods and metadata
that identify them as ``callable'',
that dynamically check their number and types of arguments,
and that can return any Python type of object,
and compiled C or Cython functions on the other hand,
which exhibit static typing.
Where it may sometimes be convenient for estimators
to take functions as arguments
(e.g., in the case of kernel machine learning,
where one might like to use a custom kernel),
but such usage is not at present possible and may never be.
Passing C or Cython-compiled functions would require an API for the kernel method
to be implemented at the C or Cython level,
only to be used from compiled code.
Passing arbitrary Python functions would be possible,
but performance can be expected to degrade significantly
because of the runtime checks needed for each invocation of the kernel function.
(The current solution in scikit-learn is to let the user pass not a kernel function,
but a precomputed Gram matrix for an entire set of objects.
The linear, polynomial and RBF kernels are treated as special cases in
the \textsf{LIBSVM}-based SVM learner, where they are implemented in C{}\verb!++!.)

\section{Comparison with other packages}

ToDo.

\section{Future work}

ToDo.

\bibliographystyle{plainnat}
\DeclareRobustCommand{\VAN}[3]{#3}
\bibliography{paper}

\end{document}
