\documentclass[a4paper]{article}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage[round]{natbib}

% working title, please change
\title{Python as a machine learning language:
       experiences from the scikit-learn project}

% Dutch name sorting hack as per http://tex.stackexchange.com/a/40750/2806
\DeclareRobustCommand{\VAN}[3]{#2}

\newcommand{\Xtr}{\texttt{X\_tr}}
\newcommand{\Xte}{\texttt{X\_te}}
\newcommand{\ytr}{\texttt{y\_tr}}
\newcommand{\yte}{\texttt{y\_te}}

\begin{document}

\maketitle

\section{Project vision}

\subsection{General principles}

ToDo.

\subsection{Data formats}

For a machine learning library, one of the most important decisions we had to
make is how to represent data.  Rather than reinventing the wheel, we opted for
NumPy multidimensional arrays \Citep{vanderwalt2011} for dense data and SciPy
sparse matrices for sparse data.  While those may seem like a ``bare''
representation of data when compared to more object-oriented representations
(Weka is an example of this style of representation), it brings the prime
advantage of allowing us to rely on NumPy and SciPy's \textit{vector
operations} which are often orders of magnitude faster than the corresponding
Python loops, while at the same time keeping the code short and readable.
Conversion to NumPy and SciPy formats is usually easy and many scientific users
of Python will already be used to such formats, since they are pervasive in
other scientific Python packages.  For tasks where input is likely to consist
of text files or semi-structured objects, we provide ``vectorizers'' -- objects
that efficiently convert such data to the NumPy or SciPy formats.
% FIXME: elaborate what is meant below
%Finally, offering a custom datatype for samples would require a conversion
%\textit{in any case}.

In the remainder of the paper, we use the following notation. Training data are
denoted by \Xtr ~and test data (i.e., unseen data to which the learning
algorithm need be able to generalize) by \Xte.  Following the discussion above,
we represent \Xtr ~and \Xte ~by 2-dimensional NumPy arrays or SciPy sparse
matrices. Using NumPy/SciPy's notation, \Xtr\texttt{[i, j]} denotes the
$i^{th}$ training sample's $j^{th}$ feature. For supervised learning tasks,
such as classification or regression, we denote the training and test labels by
\ytr ~and \yte, respectively. We typically store \ytr ~and \yte ~as
1-dimensional NumPy arrays (\ytr\texttt{[i]} denotes the $i^{th}$ training
sample's label value).

\section{Application programming interface (API)}

\subsection{Estimators}

% TODO describe transformer vs. classifiers/regressors

The bulk of the scikit-learn application programming interface
consists of so-called \textit{estimator} objects.
All learning algorithms for classification, regression and clustering
are offered as such objects,
but so are feature extraction, feature selection and dimensionality reduction.

Construction and application of an estimator object
proceeds in a way that is somewhat reminiscent of partial function application.
The constructor, \texttt{\_\_init\_\_}\footnote{
  Strictly speaking, \texttt{\_\_init\_\_} is Python's
  class \textit{initialization} method,
  while \texttt{\_\_new\_\_} is the constructor.
  The distinction is not relevant to the present description of scikit-learn.}
takes a set of named (hyper-)parameters
and attaches these to the estimator as public attributes.\footnote{
  The Python language does not actually distinguish between public and private
  attributes and methods,
  but by convention, any identifier prefixed with an underscore (\texttt{\_})
  is considered ``a non-public part of the API\ldots
  an implementation detail and subject to change without notice.''
  \Citep{pythontut}
  }
It never performs actual learning, nor does it even see a training set.

The workhorse of most estimators, and one of the few methods
that all estimators have in common, is called \texttt{fit}.
Calling this method causes the estimator to learn from a training set,
supplied as one or more arguments.
Its task, from an API point of view,
is to determine (via a learning algorithm) model-specific parameters
from a training set and set these as attributes on the estimator object,
which can then be used to transform data or make predictions.
As in other object-oriented machine learning libraries
(e.g.\ Weka's Java API, \citealp{hall2009weka})
no distinction is made in the type hierarchy
between what is variously called an estimator or a \textit{learner}
in the machine learning literature,
and the \textit{model} that results from applying an estimator to data.
When the \texttt{fit} method has been called,
an estimator object serves dual purpose as a model
(and is referred to as such in the documentation).

An example to clarify the partial application style
is the following code snippet.
If $X$ is a dataset in the format described below and $k$ is some integer,
then the Python code to train a $k$-means clustering model on it
can be written as
% XXX we could use Pygments for syntax highlighting
\begin{verbatim}
km = KMeans(n_clusters=k).fit(X)
\end{verbatim}
This snippet relies on the fact that the \texttt{fit} method
always returns the object it was called on (its ``\texttt{self}''),
making a method chaining style possible.

Importantly, the parameters learned by an estimator
are exposed as \textit{public} attributes on the trained object.
This facilitates model inspection
and makes it possible to train a model using scikit-learn,
export it to a (standardized or custom) external data format
and use the learned parameters in a different piece of code,
perhaps a prediction algorithm implemented in a different language
or one that combines the model parameters with other information
to build a custom model.
Of course, it also forces the developers to think carefully
about what might in a more ``black box''-style toolkit
be considered the internals of objects,
since authors are client code are allowed to rely
on the names and formats of model parameters.

A \texttt{fit} method is present even on objects
that perform seemingly mundane tasks such as vectorizing text documents
for subsequent learning.
Such classes, however, can be said to learn their vocabulary
from the training corpus,
and follow the conventions set out above.
Even those estimators that are entirely stateless and therefore
do not require a \texttt{fit} method to perform useful work,
nevertheless have such a method for consistency and composibility,
and to perform parameter validation.
Examples of this latter kind of objects are the normalizing transformer,
kernel approximation transformers
\citep{rahimi2007random, li2010random, vedaldi2010efficient},
and two feature extraction classes that implement the ``hashing trick''
\citep{weinberger2009}.

\section{Pipelines and model selection}

A distinguishing feature of scikit-learn estimators
is that they can be composed into \texttt{Pipeline} objects.
Not only do such objects combine typical machine learning workflows
(extracting features, centering/scaling/normalization,
feature selection, making predictions)
in a single object that is itself an estimator,
and whose methods cause data to flow through the pipeline.
More importantly, though, they can be used for
systematic, automated model selection.
This is best explained by example.

By composing, say, a \texttt{CountVectorizer} object
(which extracts term frequency features from text documents
according to a bag-words model),
a \textsf{tf--idf} transformer, a $\chi^2$ feature selector and a linear SVM,
one obtains a pipeline object with various hyperparameters:
whether to do stopword filtering,
linear vs.\ logarithmic \textsf{tf}, number of features to keep,
SVM regularization ($L_1$ or $L_2$ norm, strength $C$) a.o.
Such a composite \texttt{Pipeline} object
exposes all of these parameters using a special syntax;
each estimators must be given a name,
and if the SVM's name is, e.g., \texttt{linearsvm},
then its $C$ parameter is exposed as \texttt{linearsvm\_\_C}.

The aforementioned \texttt{Pipeline} object
can be passed as an argument to a model selection algorithm.
Available algorithms at the time of writing include random search
\citep{bergstra2012} and ``grid search''.
The latter, which we shall use in this example,
performs an exhaustive search through a grid formed by the Cartesian product
of a set of possible values for each parameter, specified by the user.
In our example, this grid might be given as

\begin{align*}
         & \textsf{stopwords} \in \{0, 1\}                      \\
  \times & \; \textsf{tf} \in \{\textsf{linear}, \textsf{log}\} \\
  \times & \; n_\textsf{features} \in \{1000, 2000, 5000\}      \\
  \times & \; \textsf{norm} \in \{L_1, L_2\}                    \\
  \times & \; C \in \{10, 100, 1000\}
\end{align*}

At each point in this grid, $k$-fold cross validation is run
to estimate the performance of the estimator according to some measure
(e.g.\ accuracy, $F_\beta$-score or a host of clustering quality metrics)
and the best performing set of hyperparameters is stored
on the \texttt{GridSearchCV} object; again, as a public attribute.

\section{Comparison with other packages}

ToDo.

\section{Future work}

ToDo.

\bibliographystyle{plainnat}
\DeclareRobustCommand{\VAN}[3]{#3}
\bibliography{paper}

\end{document}
