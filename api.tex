\section{Application programming interface (API)}

The bulk of the scikit-learn application programming interface
consists of so-called \textit{estimator} objects.
All learning algorithms for classification, regression and clustering
are offered as such objects,
but so are feature extraction, feature selection and dimensionality reduction.
What all estimators have in common is a method called \texttt{fit},
which causes the estimator to learn from data supplied as an argument.
Such a method may seem superfluous for tasks such as vectorizing text documents
for subsequent learning,
but even the classes that handle this task can be said
to learn their vocabulary from the training corpus.
(Only in a very small number of cases,
specifically two estimators that implement the ``hashing trick'' of
\citealt{weinberger2009} for feature extraction,
is \texttt{fit} a no-op offered only for consistency and composibility.)

The \texttt{fit} method's task is to learn model-specific parameters
from a training set and set these as attributes on the estimator object,
which can then be used to transform data or make predictions.
As in other object-oriented machine learning libraries
(e.g.\ Weka's Java API, \citealp{hall2009weka})
no distinction is made in the type hierarchy
between what is variously called an estimator or a \textit{learner}
in the machine learning literature,
and the \textit{model} that results from applying an estimator to data.
When the \texttt{fit} method has been called,
an estimator object serves dual purpose as a model
(and is referred to as such in the documentation).

Importantly, the parameters learned by an estimator
are exposed as \textit{public} attributes on the trained object.\footnote{
  The Python language does not actually distinguish between public and private
  attributes and methods,
  but by convention, any identifier prefixed with an underscore (\texttt{\_})
  is considered ``a non-public part of the API\ldots
  an implementation detail and subject to change without notice.''}
This facilitates model inspection
and makes it possible to train a model using scikit-learn,
export it to a (standardized or custom) external data format
and use the learned parameters in a different piece of code,
perhaps a prediction algorithm implemented in a different language
or one that combines the model parameters with other information
to build a custom model.
Of course, it also forces the developers to think carefully
about what might in a more ``black box''-style toolkit
be considered the internals of objects,
since authors are client code are allowed to rely
on the names and formats of model parameters.

% subsection here? also discuss scipy.sparse?
Input to an estimator is typically given as NumPy arrays \citep{vanderwalt2011};
typically, a clustering or dimensionality reduction algorithm
takes a single NumPy array, conventionally called $X$ of size $n \times m$
to represent $n$ samples of $m$ features each.
A regression or classification estimator takes, at training time,
an additional array of size $n$, by convention called $y$,
that represents the target variable.
While the NumPy array may seem like a ``bare'' representation of data
when compared to more object-oriented representations
(again, Weka is an example of this style of API),
it brings the prime advantages of allowing \textit{vector operations}
on entire arrays,
which are often orders of magnitude faster than the corresponding Python loops
while at the same time keeping the code short and readable.
Although conversion to the NumPy format may seem like a burden on the user,
we argue that this is not the case.
Python's lists (its dynamic array structure)
are easily transformed into NumPy arrays
and many scientific users of Python will already be handling such arrays,
since they pervade in other science-oriented Python packages.
For tasks where input is likely to consists of text files
or semi-structured objects, ``vectorizers'' are provided
that efficiently convert from such data formats to the NumPy format.
Finally, offering a custom datatype for samples
would require a conversion \textit{in any case}.


\subsection{Pipelines and model selection}

A distinguishing feature of scikit-learn estimators
is that they can be composed into \texttt{Pipeline} objects.
Not only do such objects combine typical machine learning workflows
(extract features, center/scale/normalize, select features, 
make predictions) in a single object that is itself an estimator
whose methods cause data to flow through the pipeline,
but more importantly they can be used for
systematic, automated model selection.
This is best explained by an example.

By composing, say, a \texttt{CountVectorizer} object
(which extracts term frequency features from text documents
according to a bag-words model),
a \textsf{tf--idf} transformer, a $\chi^2$ feature selector and a linear SVM,
one obtains a pipeline object with various hyperparameters:
whether to do stopword filtering,
linear vs.\ logarithmic \textsf{tf}, number of features to keep,
SVM regularization ($L_1$ or $L_2$ norm, strength $C$) a.o.
Such a composite \texttt{Pipeline} object
exposes all of these parameters using a special syntax;
each estimators must be given a name,
and if the SVM's name is, e.g., \texttt{linearsvm},
then its $C$ parameter is exposed as \texttt{linearsvm\_\_C}.

The aforementioned \texttt{Pipeline} object
can be passed as an argument to a model selection algorithm.
Available algorithms at the time of writing include random search
\citep{bergstra2012} and ``grid search''.
The latter, which we shall use in this example,
performs an exhaustive search through a grid formed by the Cartesian product
of a set of possible values for each parameter, specified by the user.
In our example, this grid might be given as

\begin{align*}
         & \textsf{stopwords} \in \{0, 1\}                      \\
  \times & \; \textsf{tf} \in \{\textsf{linear}, \textsf{log}\} \\
  \times & \; n_\textsf{features} \in \{1000, 2000, 5000\}      \\
  \times & \; \textsf{norm} \in \{L_1, L_2\}                    \\
  \times & \; C \in \{10, 100, 1000\}
\end{align*}

At each point is this grid, $k$-fold cross validation is run
to estimate the performance of the estimator according to some measure
(e.g.\ accuracy, $F_\beta$-score or a host of clustering quality metrics)
and the best performing set of hyperparameters is stored
on the \texttt{GridSearchCV} object; again, as a public attribute.
